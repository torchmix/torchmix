import { Callout } from "nextra-theme-docs";

# Component Class

**TorchMix** provides the `Component` class, which is a drop-in replacement for
`nn.Module` with enhanced configuration support.
Consider the following example:

```python
from torchmix import Component


class Model(Component):
    def __init__(self, a: int, b: str, c: list[str]):
        pass
```

Without `Component`, you would have to manually write
configurations, like this:

```python
from dataclasses import dataclass

@dataclass
class ModelA:
    a: int = 1
    b: str = "x"
    c: list[str] = ["foo"]

@dataclass
class ModelB:
    a: int = 2
    b: str = "y"
    c: list[str] = ["foo", "bar"]

@dataclass
class ModelC:
    a: int = 3
    b: str = "z"
    c: list[str] = ["foo", "bar", "baz"]
```

These dataclasses as configurations are so-called **Structured Configs**.
They enable both runtime and static type checking for more robust configuration management.
See the [hydra docs](https://hydra.cc/docs/tutorials/structured_config/intro/) or
[omegaconf docs](https://omegaconf.readthedocs.io/en/latest/structured_config.html) for more detail.

However, Writing these configurations manually can be time-consuming and often result in
redundant code. With `Component`, you can just simply **instantiate** your desired state -
and the configs will be just there for you:

```python
Model(a=1, b="x", c=["foo"]).config
Model(a=2, b="y", c=["foo", "bar"]).config
Model(a=3, b="z", c=["foo", "bar", "baz"]).config
```

You can then directly register these structured configs into `hydra`'s `ConfigStore`
via the `store` method.

```python
Model(a=1, b="x", c=["foo"]).store(group="model", name="a")
Model(a=2, b="y", c=["foo", "bar"]).store(group="model", name="b")
Model(a=3, b="z", c=["foo", "bar", "baz"]).store(group="model", name="c")
```

<Callout type="info">
  This feature is built on top of
  [`hydra-zen`](https://github.com/mit-ll-responsible-ai/hydra-zen). Check
  [their docs](https://mit-ll-responsible-ai.github.io/hydra-zen/) for more
  information!
</Callout>

## Nested Components

In deep learning, models are always composed of multiple sub-modules.
However, writing configurations for those nested modules can be
even more confusing and repetitive. Let's say we have these models:

```python
from torchmix import Component


class Model(Component):
    def __init__(self, a: Component, b: str, c: str):
        pass

class SubModel(Component):
    def __init__(self, i: Component, j: float, k: int):
        pass

class SubSubModel(Component):
    def __init__(self, p: int, q: int):
        pass
```

and would use these models like this:

```python
model = Model(
    SubModel(
        SubSubModel(1, 2),
        3e-4,
        32,
    ),
    "adam",
    "imagenet",
)
```

Let me write the `hydra` compatible configurations for this setting:

```python
@dataclass
class SubSubModelConfig:
    _target_: str = "your_library.SubSubModel"
    p: int = 1
    q: int = 2

@dataclass
class SubModelConfig:
    _target_: str = "your_library.SubModel"
    i: SubSubModelConfig = SubSubModelConfig()
    j: float = 3e-4
    k: int = 32

@dataclass
class ModelConfig:
    _target_: str = "your_library.Model"
    a: SubModelConfig = SubModelConfig()
    b: float = 3e-4
    c: int = 32
```

This does not seem good, right? `Component` come to the rescue.
The truth is, it was already there!

```python
model.config  # ðŸ¤¯
```

This would result in the following configuration:

```yaml
_target_: your_library.Model
a:
  _target_: your_library.SubModel
  i:
    _target_: your_library.SubSubModel
    p: 1
    q: 2
  j: 0.0003
  k: 32
b: adam
c: imagenet
```

`Component` allows you to confidently write arbitrarily nested modules
and easily integrate them into the `hydra` ecosystem, as demonstrated below:

```python
nn.Sequential(
    Attach(
        Token(dim=768),
        Add(
            PatchEmbed(patch_size=16),
            PositionEmbed(seq_length=196, dim=768),
        ),
    ),
    Repeat(
        nn.Sequential(
            PreNorm(
                Attention(
                    dim=768,
                    num_heads=12,
                    head_dim=64,
                ),
                dim=768,
            ),
            PreNorm(
                MLP(
                    dim=768,
                    act_layer=nn.GELU(),
                    expansion_factor=4,
                ),
                dim=768,
            ),
        ),
        depth=12,
    ),
    Extract(0),
    nn.Linear(768, 1000),
).store(group="model", name="vit")
```

All components provided by **TorchMix** are subclasses of the `Component` class.
We also offer `Component` version of PyTorch's `nn` module.
Just import `nn` from **TorchMix**!

```python
from torchmix import nn
```

## Duplicating Components

This auto-magic configuration generation feature has a handy side effect: you
can easily duplicate components with the exactly same configuration:

```python
from hydra.utils import instantiate
from torchmix import Component

model: Component = ...  # some complex component

model_new = instantiate(model.config)
```

You can just use the `instantiate` method of the `Component` class.

```python
model_new = model.instantiate()
```

A direct application of this feature is the [`Repeat`](/components/Repeat) container,
which creates an `nn.Sequential` with multiple instances of the same component.

In the example below, not only the `Attention` is duplicated, but also the plugins
passed as arguments, such as `CausalMask`, `RelativePositionBias`, `DropAttention`, and
`DropProjection`, are also duplicated 12 times:

```python
Repeat(
    Attention(
        dim=768,
        num_heads=12,
        head_dim=64,
        plugins=[
            CausalMask(mode="dynamic"),
            RelativePositionBias(
                seq_length=1024,
                num_buckets=256,
                num_heads=12,
                causal=True,
            ),
            DropAttention(p=0.1),
            DropProjection(p=0.1),
        ],
    ),
    depth=12
)
```

import { Callout } from "nextra-theme-docs"
import { Tab, Tabs } from "nextra-theme-docs"

# WindowAttention

Local window attention layer from [Swin Transformer](https://arxiv.org/abs/2103.14030).

```python copy
WindowAttention(dim=96, window_size=8, num_heads=8, head_dim=64)
```



## Parameters

- `dim`: The dimension size.
- `num_heads`: The number of attention heads.
- `head_dim`: The dimension size for each attention head.
- `window_size`: The window size for local attentions.
- `plugins`: A list of [`AttentionPlugin`](/plugins/AttentionPlugin)s to use.

## Forward
```rust
(x: jaxtyping.Float[Tensor, '... n d']) -> jaxtyping.Float[Tensor, '... n d']
```

import { Callout } from "nextra-theme-docs"
import { Tab, Tabs } from "nextra-theme-docs"

# Attention

Base class for all multi-head self attentions.

```python copy
Attention(dim=768, num_heads=8, head_dim=64, plugins=[])
```



## Parameters

- `dim`: The dimension size.
- `num_heads`: The number of attention heads.
- `head_dim`: The dimension size for each attention head.
- `plugins`: A list of [`AttentionPlugin`](/plugins/AttentionPlugin)s to use.

## Forward
```rust
(x: jaxtyping.Float[Tensor, '... d']) -> jaxtyping.Float[Tensor, '... d']
```
